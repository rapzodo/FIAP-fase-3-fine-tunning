# Fine-Tuning do Gemma-2B para Descri√ß√µes de Produtos da Amazon

## üìã Vis√£o Geral do Projeto

Este projeto demonstra o fine-tuning do modelo foundation Gemma-2B usando LoRA (Low-Rank Adaptation) em dados de produtos da Amazon. O modelo aprende a gerar descri√ß√µes de produtos baseadas em t√≠tulos de produtos, usando o dataset AmazonTitles-1.3MM.

**Tech Challenge - Fase 3 - FIAP**

---

## üéØ Prop√≥sito

1. **Fazer fine-tuning de um modelo foundation** (Gemma-2B) em dados espec√≠ficos de dom√≠nio (produtos da Amazon)
2. **Usar Fine-Tuning Eficiente em Par√¢metros (PEFT)** com LoRA para treinar em hardware de consumidor
3. **Comparar performance do modelo** antes e depois do fine-tuning
4. **Demonstrar recupera√ß√£o de fontes de treinamento** usando RAG (Retrieval-Augmented Generation)

---

## üèóÔ∏è Arquitetura

### Componentes:
- **Modelo Base**: Google Gemma-2B (2 bilh√µes de par√¢metros)
- **M√©todo de Fine-Tuning**: LoRA (Low-Rank Adaptation)
- **Dataset**: AmazonTitles-1.3MM (131.262 produtos com t√≠tulos e descri√ß√µes)
- **Banco de Dados Vetorial**: ChromaDB para busca sem√¢ntica
- **Embeddings**: SentenceTransformer (all-MiniLM-L6-v2)
- **Interface**: Aplica√ß√£o web Streamlit

### Fluxo de Treinamento:
```
Dataset Amazon ‚Üí Fine-Tuning LoRA ‚Üí Modelo Fine-Tunado
                       ‚Üì
                 Indexa√ß√£o ChromaDB (para refer√™ncias)
```

### Fluxo de Infer√™ncia:
```
Pergunta do Usu√°rio ‚Üí Modelo Fine-Tunado ‚Üí Descri√ß√£o Gerada
                ‚Üì
         Busca Sem√¢ntica ChromaDB ‚Üí Refer√™ncias de Treinamento (para transpar√™ncia)
```

---

## üöÄ In√≠cio R√°pido

### Pr√©-requisitos

1. **Python 3.10+**
2. **24GB+ RAM** (para carregar o modelo)
3. **Apple Silicon (M1/M2/M3/M4)** ou **GPU CUDA** (opcional, para treinamento mais r√°pido)
4. **Token Hugging Face** (necess√°rio para acesso ao modelo Gemma)
   - Obtenha o token em: https://huggingface.co/settings/tokens
   - Aceite a licen√ßa Gemma em: https://huggingface.co/google/gemma-2b

### Instala√ß√£o

```bash
# Clone o reposit√≥rio
cd FIAP-fase-3-fine-tunning

# Crie o ambiente virtual
python -m venv .venv
source .venv/bin/activate  # No Windows: .venv\Scripts\activate

# Instale as depend√™ncias
pip install -r requirements.txt
```

### Executando a Aplica√ß√£o

```bash
# Inicie a aplica√ß√£o Streamlit
streamlit run src/streamlit_app.py
```

A aplica√ß√£o abrir√° no seu navegador em `http://localhost:8501`

---

## üìä Configura√ß√£o de Treinamento

### Padr√£o (Treinamento Noturno - Recomendado para Demo)

**Otimizado para treinamento de 8-10 horas no Apple M4 Pro (24GB RAM)**

```python
max_samples = 3000      # 3.000 produtos da Amazon
epochs = 3              # Treinar por 3 √©pocas
batch_size = 2          # Batch size de 2
```

**Resultados Esperados:**
- ‚úÖ Tempo de Treinamento: ~8-10 horas
- ‚úÖ Boa cobertura de categorias de produtos
- ‚úÖ Adequado para prop√≥sitos de demonstra√ß√£o
- ‚úÖ Modelo aprende padr√µes de Q&A de produtos da Amazon

### Estimativas de Tempo de Treinamento

| Amostras | √âpocas | Batch Size | Tempo Estimado | Caso de Uso |
|----------|--------|------------|----------------|-------------|
| 500      | 3      | 2          | ~2-3 horas     | Teste r√°pido |
| 1.000    | 3      | 2          | ~3-4 horas     | Demo pequena |
| **3.000**    | **3**      | **2**          | **~8-10 horas**    | **Demo noturna** ‚úÖ |
| 5.000    | 3      | 2          | ~12-16 horas   | Demo estendida |
| 10.000   | 3      | 4          | ~24-36 horas   | Produ√ß√£o-lite |

---

## üéì Configura√ß√£o LoRA Explicada

### Configura√ß√µes Atuais (Otimizadas para Demo)

```python
LoraConfig(
    r=8,                    # Rank: n√∫mero de par√¢metros trein√°veis
    lora_alpha=32,          # Fator de escala (alpha/r = amplifica√ß√£o 4x)
    lora_dropout=0.1,       # 10% dropout para prevenir overfitting
    target_modules=[        # Quais camadas do modelo fazer fine-tune
        "q_proj", "o_proj", "k_proj", "v_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
)
```

### Detalhamento dos Par√¢metros

| Par√¢metro | Atual | Prop√≥sito | Impacto na Demo |
|-----------|-------|-----------|-----------------|
| **r** (Rank) | 8 | N√∫mero de par√¢metros trein√°veis adicionados | Bom equil√≠brio: treinamento r√°pido, capacidade decente |
| **lora_alpha** | 32 | Quanto o LoRA influencia a sa√≠da (32/8 = 4x) | Aprendizado forte dos dados da Amazon |
| **lora_dropout** | 0.1 | Previne overfitting em dados limitados | Ajuda na generaliza√ß√£o |

---

## üè≠ Fine-Tuning de Produ√ß√£o Completo (Todos os 131K Produtos)

### Configura√ß√£o Recomendada para Dataset Completo

```python
# Treinamento com dataset completo
max_samples = 131262    # Todos os produtos em trn.json
epochs = 3-5            # 3 para velocidade, 5 para melhor qualidade
batch_size = 4          # Aumente se tiver mais mem√≥ria GPU

# Configura√ß√£o LoRA aprimorada
LoraConfig(
    r=16,               # Dobrar o rank para mais capacidade
    lora_alpha=32,      # Manter amplifica√ß√£o 2x
    lora_dropout=0.05,  # Dropout menor com mais dados
    target_modules=[    # Mesmos m√≥dulos alvo
        "q_proj", "o_proj", "k_proj", "v_proj",
        "gate_proj", "up_proj", "down_proj"
    ]
)

# Argumentos de treinamento
TrainingArguments(
    num_train_epochs=3,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,  # Batch size efetivo = 16
    learning_rate=1e-4,             # LR menor para estabilidade
    warmup_steps=100,               # Aquecimento gradual
    save_strategy="steps",
    save_steps=5000,                # Salvar checkpoints a cada 5K passos
    logging_steps=100,
)
```

### Estimativas de Treinamento Completo

| Configura√ß√£o | Tempo de Treinamento | Hardware Necess√°rio |
|--------------|---------------------|---------------------|
| 131K √ó 3 √©pocas | ~3-4 dias | 24GB RAM, Apple M4 Pro |
| 131K √ó 5 √©pocas | ~5-7 dias | 24GB RAM, Apple M4 Pro |
| 131K √ó 3 √©pocas | ~12-18 horas | NVIDIA A100 40GB |

**Recomenda√ß√µes para Produ√ß√£o:**
- Use `r=16` ou `r=32` para melhor memoriza√ß√£o
- Treine por 5-10 √©pocas para qualidade de produ√ß√£o
- Implemente conjunto de avalia√ß√£o para monitorar overfitting
- Salve checkpoints a cada 5.000 passos
- Use agendamento de taxa de aprendizado (warmup + decay)

---

## üìñ Como Usar a Aplica√ß√£o

### 1. Digite o Token Hugging Face
- Cole seu token na barra lateral
- Necess√°rio para baixar o modelo Gemma-2B

### 2. Configure o Treinamento (Opcional)
Ajuste na barra lateral:
- **Max Samples**: 3.000 (padr√£o para noturno)
- **√âpocas**: 3 (padr√£o)
- **Batch Size**: 2 (seguro para 24GB RAM)

### 3. Visualize o Dataset (Opcional)
- Veja a aba "Dataset Preview"
- Confira exemplos de produtos da Amazon

### 4. Inicie o Fine-Tuning
- V√° para a aba "Fine-tuning Process"
- Clique em "Start Fine-tuning Process"
- Deixe executando durante a noite (~8-10 horas)

### 5. Compare Resultados
- V√° para a aba "Compare Before/After Fine-tuning"
- Digite uma pergunta como: "O que √© Mog's Kittens?"
- Clique em "Generate with Original Model" (antes)
- Clique em "Generate with Fine-tuned Model" (depois)
- Veja a melhoria e as refer√™ncias de treinamento!

---

## üî¨ Detalhes T√©cnicos

### LoRA (Low-Rank Adaptation)

LoRA adiciona pequenas matrizes trein√°veis √†s camadas de aten√ß√£o do modelo, permitindo fine-tuning com:
- **99,9% menos par√¢metros trein√°veis** (vs fine-tuning completo)
- **3x menos mem√≥ria** necess√°ria
- **Treinamento mais r√°pido** (horas em vez de dias)
- **F√°cil de compartilhar** (s√≥ precisa compartilhar adaptadores LoRA, n√£o o modelo completo)

### Por Que Esses Par√¢metros Funcionam

**Para Demo (3.000 amostras, r=8):**
- Treina rapidamente (durante a noite)
- Aprende estilo e formata√ß√£o de produtos da Amazon
- Suficiente para demonstrar efetividade do fine-tuning
- N√£o memorizar√° perfeitamente todos os produtos (esperado)

**Para Produ√ß√£o (131K amostras, r=16-32):**
- Cobertura abrangente de todos os produtos
- Melhor memoriza√ß√£o e recall
- Mais robusto a varia√ß√µes nas perguntas
- Qualidade pronta para produ√ß√£o

---

## üìà Resultados Esperados

### Antes do Fine-Tuning
- Respostas gen√©ricas
- Pode alucinar informa√ß√µes
- Sem conhecimento de produtos espec√≠ficos da Amazon
- Criativo mas incorreto

### Depois do Fine-Tuning (3.000 amostras)
- ‚úÖ Descri√ß√µes de produtos no estilo Amazon
- ‚úÖ Melhor formata√ß√£o e estrutura
- ‚úÖ Para de gerar perguntas de acompanhamento
- ‚úÖ Respostas mais focadas
- ‚ö†Ô∏è Pode ainda alucinar para produtos n√£o vistos (normal com treinamento limitado)

### Depois do Fine-Tuning Completo (131K amostras)
- ‚úÖ Conhecimento abrangente de produtos
- ‚úÖ Melhor recall de produtos espec√≠ficos
- ‚úÖ Descri√ß√µes mais precisas
- ‚úÖ Qualidade pronta para produ√ß√£o

---

## üõ†Ô∏è Estrutura do Projeto

```
FIAP-fase-3-fine-tunning/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ trn.json                 # Dataset Amazon (131.262 produtos)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ gemma-2b-finetuned/     # Adaptadores LoRA fine-tunados
‚îú‚îÄ‚îÄ chroma_db/                   # Banco de dados vetorial para refer√™ncias
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ streamlit_app.py        # Aplica√ß√£o UI principal
‚îÇ   ‚îú‚îÄ‚îÄ model_utils.py          # L√≥gica de fine-tuning e infer√™ncia
‚îÇ   ‚îú‚îÄ‚îÄ api_model_utils.py      # Utilit√°rios de carregamento de modelo
‚îÇ   ‚îî‚îÄ‚îÄ rag_utils.py            # Sistema RAG para refer√™ncias
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## üé¨ Criando Seu V√≠deo de Demonstra√ß√£o

### O Que Mostrar (m√°ximo 10 minutos)

1. **Introdu√ß√£o (1 min)**
   - Explique o desafio: fine-tune do Gemma em dados da Amazon
   - Mostre a pr√©via do dataset

2. **Antes do Fine-Tuning (2 min)**
   - Fa√ßa uma pergunta sobre um produto
   - Mostre a resposta gen√©rica/incorreta do modelo original

3. **Processo de Fine-Tuning (1 min)**
   - Mostre a configura√ß√£o (3.000 amostras, 3 √©pocas)
   - Explique brevemente os par√¢metros LoRA
   - (Pule o treinamento real - apenas mostre que iniciou)

4. **Depois do Fine-Tuning (3 min)**
   - Fa√ßa a mesma pergunta
   - Mostre a resposta melhorada, no estilo Amazon
   - Mostre as refer√™ncias de treinamento (prova de aprendizado)

5. **Explica√ß√£o T√©cnica (2 min)**
   - Explique LoRA e por que √© eficiente
   - Mostre os par√¢metros de treinamento usados
   - Mencione escalabilidade para o dataset completo de 131K

6. **Conclus√£o (1 min)**
   - Resuma as melhorias
   - Mencione limita√ß√µes (dados de treinamento limitados)
   - Explique caminho para produ√ß√£o (treinamento com dataset completo)

---

## ‚ö†Ô∏è Notas Importantes

### Limita√ß√µes

1. **Conjunto de treinamento pequeno (3.000)** n√£o memorizar√° perfeitamente todos os produtos
2. **Alucina√ß√£o** pode ainda ocorrer para produtos n√£o vistos
3. **LoRA r=8** √© eficiente em par√¢metros mas tem capacidade limitada
4. **Modelo √∫nico** (Gemma-2B) - n√£o √© o maior dispon√≠vel

### Isso √â Normal!

Fazer fine-tuning de 3.000 exemplos em um modelo 2B com LoRA r=8 √© uma **prova de conceito**, n√£o implanta√ß√£o em produ√ß√£o. O modelo aprende:
- ‚úÖ O padr√£o de Q&A de produtos da Amazon
- ‚úÖ O estilo e formato das descri√ß√µes
- ‚úÖ Conhecimento geral de produtos

Mas n√£o memorizar√° perfeitamente cada produto. Isso √© esperado e aceit√°vel para uma demo!

---

## üìö Refer√™ncias

- **Dataset**: [AmazonTitles-1.3MM](https://drive.google.com/file/d/12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK/view)
- **Modelo**: [Google Gemma-2B](https://huggingface.co/google/gemma-2b)
- **Paper LoRA**: [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- **Biblioteca PEFT**: [Hugging Face PEFT](https://github.com/huggingface/peft)

---

## ü§ù Conformidade com o Tech Challenge

‚úÖ **Execu√ß√£o de fine-tuning**: PEFT baseado em LoRA no Gemma-2B  
‚úÖ **Prepara√ß√£o do dataset**: T√≠tulos + descri√ß√µes de produtos da Amazon  
‚úÖ **Compara√ß√£o antes/depois**: UI Streamlit mostra ambos  
‚úÖ **Documenta√ß√£o de treinamento**: Este README + coment√°rios no c√≥digo  
‚úÖ **Refer√™ncias/Fontes**: Sistema RAG baseado em ChromaDB  
‚úÖ **Demonstra√ß√£o em v√≠deo**: Walkthrough de 10 minutos  

---